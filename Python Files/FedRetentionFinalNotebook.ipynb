{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federal Retention Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split, KFold, ShuffleSplit, GridSearchCV, cross_val_score, cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, r2_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import RidgeCV, BayesianRidge, LassoCV, RANSACRegressor, OrthogonalMatchingPursuit, ElasticNetCV, Ridge, Lasso, LinearRegression, ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from collections import defaultdict\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Importing, Exploring, and Transforming the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Agency_Full_Clean_2.csv\")\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"engagement\"] = (df[\"40\"] + df[\"69\"] + df[\"71\"])/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['attrition_rate', 'average_salary', 'percent_advance_degrees', 'percent_female', 'engagement']]\n",
    "X_small = df[['average_salary', 'percent_advance_degrees', 'percent_female', 'engagement']]\n",
    "X_big = df.drop(columns=[\"agency\", \"agency_code\", \"employment_count\", \"quit_count\", \"year\", \"attrition_rate\", \"average_service\", \"engagement\"])\n",
    "y = df.attrition_rate\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_small.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = sns.pairplot(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_features= PolynomialFeatures(degree=2)\n",
    "X_s_poly = poly_features.fit_transform(X_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_small_scaled = scaler.fit_transform(X_small)\n",
    "X_small_poly_scaled = scaler.fit_transform(X_s_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.target import FeatureCorrelation\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "features = ['average_salary', 'percent_advance_degrees', 'percent_female', 'engagement']\n",
    "vf = FeatureCorrelation(labels=features)\n",
    "vf.fit(X_small_scaled, y)      \n",
    "vf.show()     \n",
    "fig.savefig('small_features.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['average_salary', 'percent_advance_degrees', 'percent_female', 'engagement']\n",
    "vf = FeatureCorrelation(method='mutual_info-regression', labels=features)\n",
    "vf.fit(X_small_scaled, y)      \n",
    "vf.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_small_scaled.shape)\n",
    "print(X_small_poly_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_small_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_small_poly_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Creating, Testing, and Visualizing the Small Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Testing Small Scaled Model (no polynomials)\n",
    "\n",
    "ridge = RidgeCV()\n",
    "ridge.fit(X_train, y_train)\n",
    "print(\"Ridge Train Data R2 score : %.3f\" %ridge.score(X_train,y_train))\n",
    "print(\"Ridge Test Data R2 score : %.3f\" % ridge.score(X_test, y_test))\n",
    "\n",
    "en = ElasticNetCV()\n",
    "en.fit(X_train, y_train)\n",
    "print(\"ElasticNet Train Data R2 score : %.3f\" %en.score(X_train,y_train))\n",
    "print(\"ElasticNet Test Data R2 score : %.3f\" %en.score(X_test, y_test))\n",
    "\n",
    "bayes = BayesianRidge()\n",
    "bayes.fit(X_train, y_train)\n",
    "print(\"Bayes Train Data R2 score : %.3f\" %bayes.score(X_train,y_train))\n",
    "print(\"Bayes Test Data R2 score : %.3f\" %bayes.score(X_test, y_test))\n",
    "\n",
    "lasso = LassoCV()\n",
    "lasso.fit(X_train, y_train)\n",
    "print(\"Lasso Train Data R2 score : %.3f\" %lasso.score(X_train,y_train))\n",
    "print(\"Lasso Test Data R2 score : %.3f\" %lasso.score(X_test, y_test))\n",
    "\n",
    "RANSAC = RANSACRegressor()\n",
    "RANSAC.fit(X_train, y_train)\n",
    "print(\"RANSAC Train Data R2 score : %.3f\" %RANSAC.score(X_train,y_train))\n",
    "print(\"RANSAC Test Data R2 score : %.3f\" %RANSAC.score(X_test, y_test))\n",
    "\n",
    "OMP = OrthogonalMatchingPursuit()\n",
    "OMP.fit(X_train, y_train)\n",
    "print(\"OMP Train Data R2 score : %.3f\" %OMP.score(X_train,y_train))\n",
    "print(\"OMP Test Data R2 score : %.3f\" %OMP.score(X_test, y_test))\n",
    "\n",
    "SVReg = SVR()\n",
    "SVReg.fit(X_train, y_train)\n",
    "print(\"SVR Train Data R2 score : %.3f\" %SVReg.score(X_train,y_train))\n",
    "print(\"SVR Test Data R2 score : %.3f\" %SVReg.score(X_test, y_test))\n",
    "\n",
    "GBReg = GradientBoostingRegressor()\n",
    "GBReg.fit(X_train, y_train)\n",
    "print(\"GBReg Train Data R2 score : %.3f\" %GBReg.score(X_train,y_train))\n",
    "print(\"GBReg Test Data R2 score : %.3f\" %GBReg.score(X_test, y_test))\n",
    "\n",
    "RFReg = RandomForestRegressor()\n",
    "RFReg.fit(X_train, y_train)\n",
    "print(\"RFReg Train Data R2 score : %.3f\" %RFReg.score(X_train,y_train))\n",
    "print(\"RFReg Test Data R2 score : %.3f\" %RFReg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_val = []\n",
    "for K in range(12):\n",
    "    K = K+1\n",
    "    ne = KNeighborsRegressor(n_neighbors = K, weights='distance', metric='braycurtis')\n",
    "    ne.fit(X_train, y_train)\n",
    "    r2 = ne.score(X_test, y_test)  \n",
    "    r2_val.append(r2)\n",
    "    print('R2 k= ' , K ,'for test data:' , r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Sequential()\n",
    "nn.add(Dense(32, activation='relu', input_dim=4))\n",
    "nn.add(Dense(16, activation='relu'))\n",
    "nn.add(Dense(8, activation='relu'))\n",
    "nn.add(Dense(1, activation='linear'))\n",
    "\n",
    "RMSprop = tf.keras.optimizers.RMSprop(learning_rate=0.0001)\n",
    "nn.compile(loss='mean_squared_error', optimizer=RMSprop, metrics=['mean_squared_error'])\n",
    "nn.fit(X_train, y_train, epochs=70, batch_size=1, verbose=0, shuffle=True)\n",
    "preds133 = nn.predict(X_train)\n",
    "print(\"Train Data R2 score : %.3f\" % r2_score(y_train,preds133))\n",
    "preds123 = nn.predict(X_test)\n",
    "print(\"Test Data R2 score : %.3f\" % r2_score(y_test,preds123))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Testing Small Model with Polynomials\n",
    "\n",
    "ridge = RidgeCV()\n",
    "ridge.fit(X_train1, y_train1)\n",
    "print(\"Ridge Train Data R2 score : %.3f\" %ridge.score(X_train1,y_train1))\n",
    "print(\"Ridge Test Data R2 score : %.3f\" % ridge.score(X_test1, y_test1))\n",
    "\n",
    "en = ElasticNetCV()\n",
    "en.fit(X_train1, y_train1)\n",
    "print(\"ElasticNet Train Data R2 score : %.3f\" %en.score(X_train1,y_train1))\n",
    "print(\"ElasticNet Test Data R2 score : %.3f\" %en.score(X_test1, y_test1))\n",
    "\n",
    "bayes = BayesianRidge()\n",
    "bayes.fit(X_train1, y_train1)\n",
    "print(\"Bayes Train Data R2 score : %.3f\" %bayes.score(X_train1,y_train1))\n",
    "print(\"Bayes Test Data R2 score : %.3f\" %bayes.score(X_test1, y_test1))\n",
    "\n",
    "lasso = LassoCV()\n",
    "lasso.fit(X_train1, y_train1)\n",
    "print(\"Lasso Train Data R2 score : %.3f\" %lasso.score(X_train1,y_train1))\n",
    "print(\"Lasso Test Data R2 score : %.3f\" %lasso.score(X_test1, y_test1))\n",
    "\n",
    "RANSAC = RANSACRegressor()\n",
    "RANSAC.fit(X_train1, y_train1)\n",
    "print(\"RANSAC Train Data R2 score : %.3f\" %RANSAC.score(X_train1,y_train1))\n",
    "print(\"RANSAC Test Data R2 score : %.3f\" %RANSAC.score(X_test1, y_test1))\n",
    "\n",
    "OMP = OrthogonalMatchingPursuit()\n",
    "OMP.fit(X_train1, y_train1)\n",
    "print(\"OMP Train Data R2 score : %.3f\" %OMP.score(X_train1,y_train1))\n",
    "print(\"OMP Test Data R2 score : %.3f\" %OMP.score(X_test1, y_test1))\n",
    "\n",
    "SVR1 = SVR()\n",
    "SVR1.fit(X_train1, y_train1)\n",
    "print(\"SVR Train Data R2 score : %.3f\" %SVR1.score(X_train1,y_train1))\n",
    "print(\"SVR Test Data R2 score : %.3f\" %SVR1.score(X_test1, y_test1))\n",
    "\n",
    "GBReg = GradientBoostingRegressor()\n",
    "GBReg.fit(X_train1, y_train1)\n",
    "print(\"GBReg Train Data R2 score : %.3f\" %GBReg.score(X_train1,y_train1))\n",
    "print(\"GBReg Test Data R2 score : %.3f\" %GBReg.score(X_test1, y_test1))\n",
    "\n",
    "RFReg = RandomForestRegressor()\n",
    "RFReg.fit(X_train1, y_train1)\n",
    "print(\"RFReg Train Data R2 score : %.3f\" %RFReg.score(X_train1,y_train1))\n",
    "print(\"RFReg Test Data R2 score : %.3f\" %RFReg.score(X_test1, y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_val = []\n",
    "for K in range(12):\n",
    "    K = K+1\n",
    "    ne = KNeighborsRegressor(n_neighbors = K, weights='distance', metric='braycurtis')\n",
    "    ne.fit(X_train1, y_train1)\n",
    "    r2 = ne.score(X_test1, y_test1)  \n",
    "    r2_val.append(r2)\n",
    "    print('R2 k= ' , K ,'for test data:' , r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1 = Sequential()\n",
    "nn1.add(Dense(32, activation='relu', input_dim=15))\n",
    "nn1.add(Dense(16, activation='relu'))\n",
    "nn1.add(Dense(8, activation='relu'))\n",
    "nn1.add(Dense(1, activation='linear'))\n",
    "\n",
    "RMSprop = tf.keras.optimizers.RMSprop(learning_rate=0.0001)\n",
    "nn1.compile(loss='mean_squared_error', optimizer=RMSprop, metrics=['mean_squared_error'])\n",
    "nn1.fit(X_train1, y_train1, epochs=70, batch_size=1, verbose=0, shuffle=True)\n",
    "preds134 = nn1.predict(X_train1)\n",
    "print(\"Train Data R2 score : %.3f\" % r2_score(y_train1,preds134))\n",
    "preds125 = nn1.predict(X_test1)\n",
    "print(\"Test Data R2 score : %.3f\" % r2_score(y_test1,preds125))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Grid Search for Selected Models on Scaled (but not Polynomial) Data\n",
    "\n",
    "def grid_search(X, y, model, param_grid, name='', cv=KFold(n_splits=5,shuffle=True),\n",
    "                scoring='r2', verbose=False, n_jobs=-1):\n",
    "    '''Performs a grid search over param_grid and returns the best model'''\n",
    "    GSmodel = GridSearchCV(model, cv=cv, param_grid=param_grid, scoring=scoring, n_jobs=n_jobs)\n",
    "    GSmodel = GSmodel.fit(X, y)\n",
    "    if (verbose):\n",
    "        print(name+\"best params:\")\n",
    "        print(GSmodel.best_params_)\n",
    "        print(str(scoring)+\":\")\n",
    "        print(-1*GSmodel.best_score_)\n",
    "    return GSmodel.best_estimator_\n",
    "\n",
    "def make_CV_models(X, y):\n",
    "    '''performs grid searches to find all the best models for dataset X, y'''\n",
    "    model_dict = {\n",
    "            'KRR'    : grid_search(X, y, KernelRidge(), param_grid={\"alpha\": np.logspace(-10, 2, 100), \"gamma\": np.logspace(-10, -1, 50), \"kernel\" : ['rbf']}),\n",
    "            'SVR'   : grid_search(X, y, SVR(), param_grid={\"C\": np.logspace(-1, 4, 20), \"epsilon\": np.logspace(-2, 2, 20)}),\n",
    "            'Ridge' : grid_search(X, y, Ridge(), param_grid={\"alpha\": np.logspace(-6, 6, 100)} ),\n",
    "            'Lasso' : grid_search(X, y, Lasso(max_iter = 20000), param_grid={\"alpha\": np.logspace(-2, 6, 50)} ),\n",
    "            'BR'    : grid_search(X, y, BayesianRidge(), param_grid={\"alpha_1\": np.logspace(-13,-5,10),\"alpha_2\": np.logspace(-9,-3,10), \"lambda_1\": np.logspace(-10,-5,10),\"lambda_2\": np.logspace(-11,-4,10)}) ,\n",
    "            'GBoost': grid_search(X, y, GradientBoostingRegressor(), param_grid={\"n_estimators\": np.linspace(5, 350, 50).astype('int')} ),\n",
    "            'RF'    : grid_search(X, y, RandomForestRegressor(), param_grid={\"n_estimators\": np.linspace(5, 100, 30).astype('int')}, ),\n",
    "            'kNN'   : grid_search(X, y, KNeighborsRegressor(), param_grid={\"n_neighbors\": np.linspace(2,20,18).astype('int')} )}\n",
    "\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_CV_models(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = KernelRidge(alpha=0.040370172585965494, coef0=1, degree=3, gamma=0.1,\n",
    "             kernel='rbf', kernel_params=None)\n",
    "model1.fit(X_train, y_train)\n",
    "print(\"Train Data R2 score : %.3f\" %model1.score(X_train,y_train))\n",
    "print(\"Test Data R2 score : %.3f\" %model1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = SVR(C=12.742749857031335, cache_size=200, coef0=0.0, degree=3,\n",
    "     epsilon=0.7847599703514611, gamma='scale', kernel='rbf', max_iter=-1,\n",
    "     shrinking=True, tol=0.001, verbose=False)\n",
    "model2.fit(X_train, y_train)\n",
    "print(\"Train Data R2 score : %.3f\" %model2.score(X_train,y_train))\n",
    "print(\"Test Data R2 score : %.3f\" %model2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Ridge(alpha=57.223676593502205, copy_X=True, fit_intercept=True, max_iter=None,\n",
    "       normalize=False, random_state=None, solver='auto', tol=0.001)\n",
    "model3.fit(X_train, y_train)\n",
    "print(\"Train Data R2 score : %.3f\" %model3.score(X_train,y_train))\n",
    "print(\"Test Data R2 score : %.3f\" %model3.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Lasso(alpha=0.01, copy_X=True, fit_intercept=True, max_iter=20000,\n",
    "       normalize=False, positive=False, precompute=False, random_state=None,\n",
    "       selection='cyclic', tol=0.0001, warm_start=False)\n",
    "model4.fit(X_train, y_train)\n",
    "print(\"Train Data R2 score : %.3f\" %model4.score(X_train,y_train))\n",
    "print(\"Test Data R2 score : %.3f\" %model4.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = BayesianRidge(alpha_1=5.994842503189421e-12, alpha_2=0.001, alpha_init=None,\n",
    "               compute_score=False, copy_X=True, fit_intercept=True,\n",
    "               lambda_1=1e-05, lambda_2=0.0001, lambda_init=None, n_iter=300,\n",
    "               normalize=False, tol=0.001, verbose=False)\n",
    "model5.fit(X_train, y_train)\n",
    "print(\"Train Data R2 score : %.3f\" %model5.score(X_train,y_train))\n",
    "print(\"Test Data R2 score : %.3f\" %model5.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
    "                           init=None, learning_rate=0.1, loss='ls', max_depth=3,\n",
    "                           max_features=None, max_leaf_nodes=None,\n",
    "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                           min_samples_leaf=1, min_samples_split=2,\n",
    "                           min_weight_fraction_leaf=0.0, n_estimators=75,\n",
    "                           n_iter_no_change=None, presort='deprecated',\n",
    "                           random_state=None, subsample=1.0, tol=0.0001,\n",
    "                           validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "model6.fit(X_train, y_train)\n",
    "print(\"Train Data R2 score : %.3f\" %model6.score(X_train,y_train))\n",
    "print(\"Test Data R2 score : %.3f\" %model6.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
    "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "                       max_samples=None, min_impurity_decrease=0.0,\n",
    "                       min_impurity_split=None, min_samples_leaf=1,\n",
    "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                       n_estimators=96, n_jobs=None, oob_score=False,\n",
    "                       random_state=None, verbose=0, warm_start=False)\n",
    "model7.fit(X_train, y_train)\n",
    "print(\"Train Data R2 score : %.3f\" %model7.score(X_train,y_train))\n",
    "print(\"Test Data R2 score : %.3f\" %model7.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                     metric_params=None, n_jobs=None, n_neighbors=12, p=2,\n",
    "                     weights='uniform')\n",
    "model8.fit(X_train, y_train)\n",
    "print(\"Train Data R2 score : %.3f\" %model8.score(X_train,y_train))\n",
    "print(\"Test Data R2 score : %.3f\" %model8.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small Model Conclusion -- Best Model = KNeighbors Regression\n",
    "\n",
    "small_model = KNeighborsRegressor(n_neighbors = 5, weights='distance', metric='braycurtis')\n",
    "small_model.fit(X_train, y_train)\n",
    "print(\"Train Data R2 score : %.3f\" %small_model.score(X_train,y_train))\n",
    "print(\"Test Data R2 score : %.3f\" %small_model.score(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import PredictionError\n",
    "\n",
    "fig = plt.figure()\n",
    "v1 = PredictionError(small_model)\n",
    "v1.fit(X_train, y_train)\n",
    "v1.score(X_test, y_test)\n",
    "v1.show()\n",
    "fig.savefig('small_PE.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from yellowbrick.model_selection import CVScores\n",
    "\n",
    "fig = plt.figure()\n",
    "cv = KFold(n_splits=10, random_state=42)\n",
    "v2 = CVScores(small_model, cv=cv, scoring='r2')\n",
    "v2.fit(X_train, y_train)\n",
    "v2.show()  \n",
    "fig.savefig('small_CV.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import ResidualsPlot\n",
    "\n",
    "fig = plt.figure()\n",
    "v3 = ResidualsPlot(small_model)\n",
    "v3.fit(X_train, y_train) \n",
    "v3.score(X_test, y_test) \n",
    "v3.show() \n",
    "fig.savefig('small_Res.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "fig = plt.figure()\n",
    "v4 = LearningCurve(small_model, scoring='r2')\n",
    "v4.fit(X_train, y_train)      \n",
    "v4.show()  \n",
    "fig.savefig('small_LC.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import ValidationCurve\n",
    "\n",
    "param_range = np.arange(1, 10, 1)\n",
    "v5 = ValidationCurve(small_model, param_name=\"n_neighbors\",\n",
    "    param_range=param_range, cv=6, scoring=\"r2\", n_jobs=-1)\n",
    "v5.fit(X_train, y_train)\n",
    "v5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import CooksDistance\n",
    "\n",
    "v6 = CooksDistance()\n",
    "v6.fit(X_train, y_train)\n",
    "v6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Creating, Testing, and Visualizing the Big Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_big_scaled = scaler.fit_transform(X_big)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_big_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_big.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = ['average_salary', 'percent_advance_degrees', 'percent_female', '1', '2',\n",
    "       '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15',\n",
    "       '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27',\n",
    "       '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39',\n",
    "       '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51',\n",
    "       '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63',\n",
    "       '64', '65', '66', '67', '68', '69', '70', '71']\n",
    "\n",
    "fig = plt.figure(figsize=(10,16))\n",
    "vf = FeatureCorrelation(labels=features1)\n",
    "vf.fit(X_big_scaled, y)      \n",
    "vf.show()\n",
    "fig.savefig('big_features.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Testing Large Scaled Model (no polynomials)\n",
    "\n",
    "ridge1 = RidgeCV()\n",
    "ridge1.fit(X_train, y_train)\n",
    "print(\"Ridge Train Data R2 score : %.3f\" %ridge1.score(X_train,y_train))\n",
    "print(\"Ridge Test Data R2 score : %.3f\" % ridge1.score(X_test, y_test))\n",
    "\n",
    "en1 = ElasticNetCV()\n",
    "en1.fit(X_train, y_train)\n",
    "print(\"ElasticNet Train Data R2 score : %.3f\" %en1.score(X_train,y_train))\n",
    "print(\"ElasticNet Test Data R2 score : %.3f\" %en1.score(X_test, y_test))\n",
    "\n",
    "bayes1 = BayesianRidge()\n",
    "bayes1.fit(X_train, y_train)\n",
    "print(\"Bayes Train Data R2 score : %.3f\" %bayes1.score(X_train,y_train))\n",
    "print(\"Bayes Test Data R2 score : %.3f\" %bayes1.score(X_test, y_test))\n",
    "\n",
    "lasso1 = LassoCV()\n",
    "lasso1.fit(X_train, y_train)\n",
    "print(\"Lasso Train Data R2 score : %.3f\" %lasso1.score(X_train,y_train))\n",
    "print(\"Lasso Test Data R2 score : %.3f\" %lasso1.score(X_test, y_test))\n",
    "\n",
    "RANSAC1 = RANSACRegressor()\n",
    "RANSAC1.fit(X_train, y_train)\n",
    "print(\"RANSAC Train Data R2 score : %.3f\" %RANSAC1.score(X_train,y_train))\n",
    "print(\"RANSAC Test Data R2 score : %.3f\" %RANSAC1.score(X_test, y_test))\n",
    "\n",
    "OMP1 = OrthogonalMatchingPursuit()\n",
    "OMP1.fit(X_train, y_train)\n",
    "print(\"OMP Train Data R2 score : %.3f\" %OMP1.score(X_train,y_train))\n",
    "print(\"OMP Test Data R2 score : %.3f\" %OMP1.score(X_test, y_test))\n",
    "\n",
    "SVReg1 = SVR()\n",
    "SVReg1.fit(X_train, y_train)\n",
    "print(\"SVR Train Data R2 score : %.3f\" %SVReg1.score(X_train,y_train))\n",
    "print(\"SVR Test Data R2 score : %.3f\" %SVReg1.score(X_test, y_test))\n",
    "\n",
    "GBReg1 = GradientBoostingRegressor()\n",
    "GBReg1.fit(X_train, y_train)\n",
    "print(\"GBReg Train Data R2 score : %.3f\" %GBReg1.score(X_train,y_train))\n",
    "print(\"GBReg Test Data R2 score : %.3f\" %GBReg1.score(X_test, y_test))\n",
    "\n",
    "RFReg1 = RandomForestRegressor()\n",
    "RFReg1.fit(X_train, y_train)\n",
    "print(\"RFReg Train Data R2 score : %.3f\" %RFReg1.score(X_train,y_train))\n",
    "print(\"RFReg Test Data R2 score : %.3f\" %RFReg1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_val1 = []\n",
    "for K in range(12):\n",
    "    K = K+1\n",
    "    ne1 = KNeighborsRegressor(n_neighbors = K, weights='distance', metric='braycurtis')\n",
    "    ne1.fit(X_train, y_train)\n",
    "    r2 = ne1.score(X_test, y_test)  \n",
    "    r2_val1.append(r2)\n",
    "    print('R2 k= ' , K ,'for test data:' , r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_CV_models(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_model = SVR(C=6.951927961775605, cache_size=200, coef0=0.0, degree=3,\n",
    "     epsilon=0.016237767391887217, gamma='scale', kernel='rbf', max_iter=-1,\n",
    "     shrinking=True, tol=0.001, verbose=False)\n",
    "big_model.fit(X_train, y_train)\n",
    "print(\"Train Data R2 score : %.3f\" %big_model.score(X_train,y_train))\n",
    "print(\"Test Data R2 score : %.3f\" %big_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_big = Sequential()\n",
    "nn_big.add(Dense(32, activation='relu', input_dim=74))\n",
    "nn_big.add(Dropout(0.2))\n",
    "nn_big.add(Dense(16, activation='relu'))\n",
    "nn_big.add(Dropout(0.2))\n",
    "nn_big.add(Dense(8, activation='relu'))\n",
    "nn_big.add(Dense(1))\n",
    "\n",
    "Adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "nn_big.compile(loss='mean_squared_error', optimizer=Adam, metrics=['mean_squared_error'])\n",
    "model_output = nn_big.fit(X_train, y_train, epochs=30, batch_size=1, verbose=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = nn_big.predict(X_train)\n",
    "pt1 = nn_big.predict(X_test)\n",
    "print(\"R-Squared for the Training Data: %.3f\" % r2_score(y_train,pt))\n",
    "print(\"R-Squared for the Test Data: %.3f\" % r2_score(y_test,pt1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "v1 = PredictionError(big_model)\n",
    "v1.fit(X_train, y_train)\n",
    "v1.score(X_test, y_test)\n",
    "v1.show()\n",
    "fig.savefig('big_PE.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10, random_state=42)\n",
    "v2 = CVScores(big_model, cv=cv, scoring='r2')\n",
    "v2.fit(X_train, y_train)\n",
    "v2.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "v3 = ResidualsPlot(big_model)\n",
    "v3.fit(X_train, y_train) \n",
    "v3.score(X_test, y_test) \n",
    "v3.show() \n",
    "fig.savefig('big_Res.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "v4 = LearningCurve(big_model, scoring='r2')\n",
    "v4.fit(X_train, y_train)      \n",
    "v4.show()  \n",
    "fig.savefig('big_LC.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v6 = CooksDistance()\n",
    "v6.fit(X_train, y_train)\n",
    "v6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: H2O AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = h2o.H2OFrame(df1)\n",
    "trainA, testA = frame.split_frame(ratios=[.8])\n",
    "x = frame.columns\n",
    "y = \"attrition_rate\"\n",
    "x.remove(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml = H2OAutoML(max_runtime_secs=1500, seed=1, verbosity=\"info\")\n",
    "aml.train(x=x, y=y, training_frame=trainA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = aml.leaderboard\n",
    "lb.head(rows=lb.nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml.leader.r2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
